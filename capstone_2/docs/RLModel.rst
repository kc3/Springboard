
Reinforcement Learning Model
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Sequence to Sequence Models are trained based on Maximum Likelihood
Principle, which gives the most probable answer. The model lacks some
belief concerning the appearance of a token in the vocabulary, hence a
model trained with maximum likelihood will tend to output short general
answers that are very common in the vocabulary, like yes, no, maybe,
which is a huge problem for conversation diversity. This lack of
diversity leads to repetive and short-sighted ‘stop’ answers that end
the conversation.

Reinforcement learning model is one non maximum likelihood based
approach to solve this problem of dull, repetitive answers.

Model Setup
^^^^^^^^^^^

The learning system consists of two agents. We use :math:`p` to denote
sentences generated from the first agent and :math:`q` to denote
sentences from the second. The two agents take turns talking with each
other.

A dialogue can be represented as an alternating sequence of sentences
generated by the two agents: :math:`p_1, q_1, p_2, q_2, ..., p_i, q_i`.
We view the generated sentences as actions that are taken according to a
policy defined by an encoder-decoder recurrent neural network language
model. The parameters of the network are optimized to maximize the
expected future reward using policy gradient search method.

Policy gradient methods are more appropriate for this scenario than
Q-learning, because the encoder-decoder RNN is initialized using MLE
parameters that already produce plausible responses, before changing the
objective and tuning towards a policy that maximizes long-term reward.

Q-learning, on the other hand, directly estimates the future expected
reward of each action, which can differ from the MLE objective by orders
of magnitude, thus making MLE parameters inappropriate for
initialization.

The components (states, actions, reward, etc.) of this sequential
decision problem are summarized as follows:

State
^^^^^

The state is denoted by the previous dialog turn, either :math:`p_i` or
:math:`q_i`. A vector representation of this dialog is retrieved by
feeding it into the LSTM Encoder part of the Seq to Seq model.

Action
^^^^^^

An action :math:`a` is the dialogue utterance to generate. The action
space is infinite since arbitrary-length sequences can be generated.

Policy
^^^^^^

A policy takes the form of an LSTM encoder-decoder > pRL(pi+1|qi) ) and
is defined by its parameters.

Note that a stochastic representation of the policy (a probability
distribution over actions given states) is used. A deterministic policy
would result in a discontinuous objective that is difficult to optimize
using gradient-based methods.

Reward
^^^^^^

:math:`r` denotes the reward obtained for each action. In this
subsection, we discuss major factors that contribute to the success of a
dialogue and describe how approximations to these factors can be
operationalized in computable reward functions.

Ease of Answering
'''''''''''''''''

A turn generated by a machine should be easy to respond to. This aspect
of a turn is related to its forward-looking function: the constraints a
turn places on the next turn.

A proposed measure for the ease of answering for a generated turn is
using the negative log likelihood of responding to that utterance with a
dull response. A list :math:`S` of dull responses is manually
constructed a list of dull responses consisting turns such as > “I don’t
know what you are talking about”,

   “I have no idea”,

that have bee found occur very frequently in SEQ2SEQ models of
conversations.

The reward function is given as follows: >
:math:`r_1 = -\frac{1}{N_S} \sum_{s \in S} \frac{1}{N_s} log p_{seq2seq}(s|a)`

where: > :math:`N_S` is the cardinality of the turn in :math:`S`

   :math:`N_s` is the cardinality of tokens in common with
   :math:`s \in S`

..

   :math:`p_{seq2seq}` represents the MLE output of the Seq2Seq model

Information flow
''''''''''''''''

We want each agent to contribute new information at each turn to keep
the dialogue moving and avoid repetitive sequences. We therefore
penalize semantic similarity between consecutive turns from the same
agent.

Let :math:`h_{p_i}` and :math:`h_{p_{i+1}}` denote representations
obtained from the encoder for two consecutive turns :math:`p_i` and
:math:`p_{i+1}`.

The reward is given by the negative log of the cosine similarity between
them: > :math:`r_2` = - log cos(\ :math:`h_{p_i}`, :math:`h_{p_{i+1}}`)

Semantic Coherance
''''''''''''''''''

We also need to measure the adequacy of responses to avoid situations in
which the generated replies are highly rewarded but are ungrammatical or
not coherent. We therefore consider the mutual information between the
action :math:`a` and previous turns in the history to ensure the
generated responses are coherent and appropriate: > :math:`r3` =
:math:`\frac{1}{N_a} log p_{seq2seq}(a|q_i)` +
:math:`\frac{1}{N_{q_i}} log p_{seq2seq}^{backward}(q_i|a)`

where: > :math:`log p_{seq2seq}(a|q_i)` is the probability of generating
:math:`a` given :math:`q_i`

   :math:`log p_{seq2seq}^{backward}(q_i|a)` is the probability of
   :math:`q_i` being the preceding char given that :math:`a` follows.

The second probability is the output of the backward cell of the
Bidirectional RNN used for training. Both the above probabilities are
scaled with respect to their length.

The final reward is a weighted sum of the above probabilities with
weights (0.25, 0.25, 0.5).

Curriculum Learning Strategy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A curriculum Learning strategy is employed in which we begin by
simulating the dialogue for 2 turns, and gradually increase the number
of simulated turns. We generate 5 turns at most, as the number of
candidates to examine grows exponentially in the size of candidate list.
Five candidate responses are generated at each step of the simulation.

We use Beam Search Decoder to generate a candidate list of
responses/actions :math:`a` and track rewards across the conversation.

Policy Gradient Search
^^^^^^^^^^^^^^^^^^^^^^

The key idea underlying policy gradients is to push up the probabilities
of actions that lead to higher return, and push down the probabilities
of actions that lead to lower return, until you arrive at the optimal
policy.

The goal of training is to find the parameters of the agent that
maximize the expected reward. We define our loss :math:`L` given
parameters :math:`\theta` as the negative expected reward: >
:math:`L_\theta` =
:math:`\sum_{w_1^g,..., w_T^g} p_{\theta}(w_1^g,..., w_T^g) r(w_1^g,..., w_T^g)`

which is simple sum product over probability of picking an action with
its associated reward.

The partial derivative of this loss function over the outputs of the
softmax classifier is the same as that of the multiclass logistic
regression classifier, which is the difference between the prediction
and the actual 1-of-N representation of the target word.

   :math:`\frac{\partial L_\theta}{\partial o_t}` =
   :math:`\sum_{w_1^g,..., w_T^g} (p_{\theta}(w_1^g,..., w_T^g) - 1) (r(w_1^g,..., w_T^g) - \bar{r}_{t+1})`

where :math:`\bar{r}_{t+1}` is the expected reward.

The general algorithm for policy gradient is given as:


.. image:: output_25_0.png



Policy Gradient Method trains a stochastic policy in an on-policy way.
This means that it explores by sampling actions according to the latest
version of its stochastic policy. The amount of randomness in action
selection depends on both initial conditions and the training procedure.
Over the course of training, the policy typically becomes progressively
less random, as the update rule encourages it to exploit rewards that it
has already found. This may cause the policy to get trapped in local
optima.
