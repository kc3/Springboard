{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first look at accuracy metrics and try to identify what is mis-classified most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path\n",
    "PROJ_ROOT = os.pardir\n",
    "sys.path.append(PROJ_ROOT)\n",
    "from src.features.tree import Tree\n",
    "from src.models.data_manager import DataManager\n",
    "from src.models.rntn import RNTN\n",
    "from src.features.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model \n",
    "model_name = 'RNTN_30_tanh_35_5_None_50_0.001_0.01_9645'\n",
    "\n",
    "# Instantiate model\n",
    "model_rntn = RNTN(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data for full tree\n",
    "x_test = DataManager().x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sub-phrases for a single tree\n",
    "def get_phrases(node):\n",
    "    if node.isLeaf:\n",
    "        return (np.asarray([str(node)]), np.asarray([node.label]))\n",
    "    else:\n",
    "        left_phrases, left_labels = get_phrases(node.left)\n",
    "        right_phrases, right_labels = get_phrases(node.right)\n",
    "        curr_phrases = np.concatenate([np.asarray([str(node)]), left_phrases, right_phrases])\n",
    "        curr_labels = np.concatenate([np.asarray([node.label]), left_labels, right_labels])\n",
    "        return (curr_phrases, curr_labels)\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    X_tree, y_tree = get_phrases(x_test[i].root)\n",
    "    X_data = np.concatenate([X_data, X_tree])\n",
    "    y_data = np.concatenate([y_data, y_tree])\n",
    "\n",
    "dt_test = pd.DataFrame(data={'phrase': X_data})\n",
    "dt_test.to_csv('../src/data/processed/test_phrases_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trees_data = [Tree(t) for t in X_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Tree Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data contains the each sentence and its sub-phrase and associated ground truth label. We use the model predict function to look at how each node is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\cskap\\github\\Springboard\\capstone_1\\src\\models\\../../models//RNTN_30_tanh_35_5_None_50_0.001_0.01_9645/RNTN_30_tanh_35_5_None_50_0.001_0.01_9645.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Call models predict method\n",
    "y_pred = model_rntn.predict(np.asarray(X_trees_data).reshape(-1, 1))\n",
    "y_true = y_data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\cskap\\github\\Springboard\\capstone_1\\src\\models\\../../models//RNTN_30_tanh_35_5_None_50_0.001_0.01_9645/RNTN_30_tanh_35_5_None_50_0.001_0.01_9645.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Calculate the probabilities for \n",
    "y_probs = model_rntn.predict_proba(np.asarray(X_trees_data).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on full test data (RNTN):     0.664661\n"
     ]
    }
   ],
   "source": [
    "# Accuracy \n",
    "print(\"Accuracy on full test data (RNTN):     {:2f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*RNTN model accuracy is less than the accuracy of Naive Bayes model.* Lets look closer at what is mis-classified and also compute other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.29      0.31      2008\n",
      "           1       0.38      0.46      0.42      9255\n",
      "           2       0.88      0.76      0.82     56548\n",
      "           3       0.30      0.44      0.36     10998\n",
      "           4       0.48      0.54      0.51      3791\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     82600\n",
      "   macro avg       0.47      0.50      0.48     82600\n",
      "weighted avg       0.71      0.66      0.68     82600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start seeing why this model does better with predicting sentiments over Naive Bayes. Even though the accuracy is lower, the per-class model is less confused about classification. It is not classifying everything is neutral, rather the positive sentiments are mostly misclassified as slightly positive, which will make prediction more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  591   819   250   288    60]\n",
      " [  791  4270  2178  1810   206]\n",
      " [  294  4601 43158  8036   459]\n",
      " [  116  1183  3355  4852  1492]\n",
      " [   36   219   337  1169  2030]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6836731505540418\n"
     ]
    }
   ],
   "source": [
    "# F1-score\n",
    "print(f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0353974476756531\n"
     ]
    }
   ],
   "source": [
    "# Log loss per sample\n",
    "print(log_loss(y_true, y_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see a better F1 score with RNTN model due to better classification in minority classes. The average Log loss is slightly higher, due to lesser accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root Level Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\cskap\\github\\Springboard\\capstone_1\\src\\models\\../../models//RNTN_30_tanh_35_5_None_50_0.001_0.01_9645/RNTN_30_tanh_35_5_None_50_0.001_0.01_9645.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Call models predict method\n",
    "y_pred = model_rntn.predict(np.asarray(x_test).reshape(-1,1))\n",
    "y_true = [t.root.label for t in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\cskap\\github\\Springboard\\capstone_1\\src\\models\\../../models//RNTN_30_tanh_35_5_None_50_0.001_0.01_9645/RNTN_30_tanh_35_5_None_50_0.001_0.01_9645.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Calculate probabilities for log loss\n",
    "y_probs = model_rntn.predict_proba(np.asarray(x_test).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on root test data (RNTN):     0.373756\n"
     ]
    }
   ],
   "source": [
    "# Accuracy \n",
    "print(\"Accuracy on root test data (RNTN):     {:2f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root accuracy for RNTN is significantly higher than baseline. This can be explained as extreme sentiments are not misclassified as neutral as much as the nearer class of slightly positive/negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.34      0.31       279\n",
      "           1       0.42      0.48      0.45       633\n",
      "           2       0.23      0.01      0.01       389\n",
      "           3       0.29      0.30      0.29       510\n",
      "           4       0.44      0.68      0.54       399\n",
      "\n",
      "   micro avg       0.37      0.37      0.37      2210\n",
      "   macro avg       0.33      0.36      0.32      2210\n",
      "weighted avg       0.34      0.37      0.33      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 96 120   2  40  21]\n",
      " [140 301   4 130  58]\n",
      " [ 54 141   3 124  67]\n",
      " [ 32 119   4 153 202]\n",
      " [ 11  33   0  82 273]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33485051111248126\n"
     ]
    }
   ],
   "source": [
    "# F1-score\n",
    "print(f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9482420690090614\n"
     ]
    }
   ],
   "source": [
    "# Log loss per sample\n",
    "print(log_loss(y_true, y_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All metrics show better performance as compared to root sentiment predictions for the baseline model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
