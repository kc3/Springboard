{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at bag of words models as baseline models for comparison with the RNTN model. The two models considered here are *Naive Bayes* and *Support Vector Machine* models.\n",
    "\n",
    "We evaluate the models for both root level and full tree node accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Phrases from the Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sentiment Treebank dataset is in form of parsed trees. Here we generate all sub-phrases and their associated sentiments for evaluating full accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to model code\n",
    "PROJ_ROOT = os.pardir\n",
    "sys.path.append(PROJ_ROOT)\n",
    "from src.features.tree import Tree\n",
    "from src.models.data_manager import DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sub-phrases for a single tree\n",
    "def get_phrases(node):\n",
    "    if node.isLeaf:\n",
    "        return (np.asarray([node.word]), np.asarray([node.label]))\n",
    "    else:\n",
    "        left_phrases, left_labels = get_phrases(node.left)\n",
    "        right_phrases, right_labels = get_phrases(node.right)\n",
    "        curr_phrases = np.concatenate([np.asarray([node.text()]), left_phrases, right_phrases])\n",
    "        curr_labels = np.concatenate([np.asarray([node.label]), left_labels, right_labels])\n",
    "        return (curr_phrases, curr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parsed trees\n",
    "trees_path = '../src/data/interim/trainDevTestTrees_PTB/trees/'\n",
    "x_train = DataManager(trees_path).x_train\n",
    "x_dev = DataManager(trees_path).x_dev\n",
    "x_test = DataManager(trees_path).x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sub-phrases for every tree\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(x_train)):\n",
    "    X_tree, y_tree = get_phrases(x_train[i].root)\n",
    "    X = np.concatenate([X, X_tree])\n",
    "    y = np.concatenate([y, y_tree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sub-phrases for every cross validation set tree\n",
    "X_data_dev = []\n",
    "y_data_dev = []\n",
    "for i in range(len(x_dev)):\n",
    "    X_tree, y_tree = get_phrases(x_dev[i].root)\n",
    "    X_data_dev = np.concatenate([X_data_dev, X_tree])\n",
    "    y_data_dev = np.concatenate([y_data_dev, y_tree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_data = vectorizer.fit_transform(np.concatenate([X, X_data_dev]))\n",
    "X_data = X_data.tocsc()  # some versions of sklearn return COO format\n",
    "y_data = np.concatenate([y, y_data_dev])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Predefined split as train, dev data is already separate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "\n",
    "# Prepare data for training\n",
    "validation_set_indexes = [-1] * len(X) + [0] * len(X_data_dev)\n",
    "cv = PredefinedSplit(test_fold=validation_set_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple naive bayes classifier\n",
    "from sklearn.metrics import make_scorer, log_loss, accuracy_score\n",
    "\n",
    "# Use MultinomialNB classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Find the best hyper-parameter using GridSearchCV\n",
    "params = {'alpha': [.1, 1, 5, 10, 50]}\n",
    "model = GridSearchCV(clf, params, scoring=make_scorer(accuracy_score), cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit(X_data.toarray(), y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sub-phrases for every test set tree\n",
    "X_data_test = []\n",
    "y_data_test = []\n",
    "for i in range(len(x_test)):\n",
    "    X_tree, y_tree = get_phrases(x_test[i].root)\n",
    "    X_data_test = np.concatenate([X_data_test, X_tree])\n",
    "    y_data_test = np.concatenate([y_data_test, y_tree])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "X_data_test_vec = vectorizer.fit_transform(X_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score model\n",
    "# Print the accuracy on the test and training dataset\n",
    "#training_accuracy = model.score(X_data.reshape(-1,1), y_data)\n",
    "test_accuracy = model.score(X_data_test_vec.toarray(), y_data_test.astype(int))\n",
    "\n",
    "#print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on test data:     {:2f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
