{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at bag of words models as baseline models for comparison with the RNTN model.\n",
    "\n",
    "We evaluate the models for both root level and full tree node accuracy scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Phrases from the Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sentiment Treebank dataset is in form of parsed trees. Here we generate all sub-phrases and their associated sentiments for evaluating full accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to model code\n",
    "PROJ_ROOT = os.pardir\n",
    "sys.path.append(PROJ_ROOT)\n",
    "from src.features.tree import Tree\n",
    "from src.models.data_manager import DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sub-phrases for a single tree\n",
    "def get_phrases(node):\n",
    "    if node.isLeaf:\n",
    "        return (np.asarray([node.word]), np.asarray([node.label]))\n",
    "    else:\n",
    "        left_phrases, left_labels = get_phrases(node.left)\n",
    "        right_phrases, right_labels = get_phrases(node.right)\n",
    "        curr_phrases = np.concatenate([np.asarray([node.text()]), left_phrases, right_phrases])\n",
    "        curr_labels = np.concatenate([np.asarray([node.label]), left_labels, right_labels])\n",
    "        return (curr_phrases, curr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parsed trees\n",
    "trees_path = '../src/data/interim/trainDevTestTrees_PTB/trees/'\n",
    "x_train = DataManager(trees_path).x_train\n",
    "x_dev = DataManager(trees_path).x_dev\n",
    "x_test = DataManager(trees_path).x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sub-phrases for every tree\n",
    "X_data_train = []\n",
    "y_data_train = []\n",
    "for i in range(len(x_train)):\n",
    "    X_tree, y_tree = get_phrases(x_train[i].root)\n",
    "    X_data_train = np.concatenate([X_data_train, X_tree])\n",
    "    y_data_train = np.concatenate([y_data_train, y_tree])\n",
    "    \n",
    "dt_train = pd.DataFrame(data={'phrase': X_data_train, 'label': y_data_train})\n",
    "dt_train.to_csv('../src/data/processed/train_phrases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or run only the following\n",
    "dt_train = pd.read_csv('../src/data/processed/train_phrases.csv')\n",
    "X_data_train = np.ravel(dt_train[['phrase']])\n",
    "y_data_train = np.ravel(dt_train[['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sub-phrases for every cross validation set tree\n",
    "X_data_dev = []\n",
    "y_data_dev = []\n",
    "for i in range(len(x_dev)):\n",
    "    X_tree, y_tree = get_phrases(x_dev[i].root)\n",
    "    X_data_dev = np.concatenate([X_data_dev, X_tree])\n",
    "    y_data_dev = np.concatenate([y_data_dev, y_tree])\n",
    "\n",
    "dt_dev = pd.DataFrame(data={'phrase': X_data_dev, 'label': y_data_dev})\n",
    "dt_dev.to_csv('../src/data/processed/dev_phrases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or run only the following\n",
    "dt_dev = pd.read_csv('../src/data/processed/dev_phrases.csv')\n",
    "X_data_dev = np.ravel(dt_dev[['phrase']])\n",
    "y_data_dev = np.ravel(dt_dev[['label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building vocabulary\n",
    "\n",
    "The vocabulary is built using a CountVectorizer that extracts words and pre-processes them (lemmatization). The fit_transform method returns the one-hot encoded version of the sentences with the frequency of the word occurence as the components of the generated sentence vector (rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary using CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_data = vectorizer.fit_transform(np.concatenate([X_data_train, X_data_dev]))\n",
    "X_data = X_data.tocsc()  # some versions of sklearn return COO format\n",
    "y_data = np.concatenate([y_data_train, y_data_dev])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "\n",
    "The dev/train split is already specified in the trained dataset. Here we use Predefined Split to specify which data is cross-validation test set and which is training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Predefined split as train, dev data is already separate\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "\n",
    "# Prepare data for training\n",
    "validation_set_indexes = [-1] * len(X_data_train) + [0] * len(X_data_dev)\n",
    "cv = PredefinedSplit(test_fold=validation_set_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Model\n",
    "\n",
    "The Naive Bayes model provides a good baseline as it only makes independence assumption. However, we do not expect it to do well against sentences with negation as it does not take structure of the sentence in account. It will also mark sentences with higher positive word counts more positively and similarly negative word counts will give negative sentiment as the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple naive bayes classifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, log_loss\n",
    "\n",
    "# Use MultinomialNB classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_clf = MultinomialNB()\n",
    "\n",
    "# Find the best hyper-parameter using GridSearchCV\n",
    "params = {'alpha': [.1, 1, 10]}\n",
    "nb_model = GridSearchCV(nb_clf, params, scoring=make_scorer(accuracy_score), cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1}\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "nb_model.fit(X_data, y_data)\n",
    "print(nb_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the test dataset to compare. Phrases are extracted out of each sentence as we know their sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sub-phrases for every test set tree\n",
    "X_data_test = []\n",
    "y_data_test = []\n",
    "for i in range(len(x_test)):\n",
    "    X_tree, y_tree = get_phrases(x_test[i].root)\n",
    "    X_data_test = np.concatenate([X_data_test, X_tree])\n",
    "    y_data_test = np.concatenate([y_data_test, y_tree])\n",
    "    \n",
    "dt_test = pd.DataFrame(data={'phrase': X_data_test, 'label': y_data_test})\n",
    "dt_test.to_csv('../src/data/processed/test_phrases.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or run only the following\n",
    "dt_test = pd.read_csv('../src/data/processed/test_phrases.csv')\n",
    "X_data_test = np.ravel(dt_test[['phrase']])\n",
    "y_data_test = np.ravel(dt_test[['label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a word frequency count vector for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "X_data_test_vec = vectorizer.transform(X_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on full test data (NB):     0.735557\n"
     ]
    }
   ],
   "source": [
    "# Score model\n",
    "# Print the accuracy on the test and training dataset\n",
    "#training_accuracy = model.score(X_data.reshape(-1,1), y_data)\n",
    "#test_accuracy = model.score(X_data_test_vec, y_data_test.astype(int))\n",
    "y_pred = nb_model.predict(X_data_test_vec)\n",
    "y_true = y_data_test.astype(int)\n",
    "y_probs = nb_model.predict_proba(X_data_test_vec)\n",
    "\n",
    "#print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on full test data (NB):     {:2f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gives a good accuracy score. Next we look at where the misclassifications are happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.09      0.15      2008\n",
      "           1       0.57      0.19      0.29      9255\n",
      "           2       0.76      0.97      0.86     56548\n",
      "           3       0.54      0.29      0.38     10998\n",
      "           4       0.60      0.17      0.26      3791\n",
      "\n",
      "   micro avg       0.74      0.74      0.74     82600\n",
      "   macro avg       0.58      0.34      0.39     82600\n",
      "weighted avg       0.70      0.74      0.68     82600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6833294980935176\n"
     ]
    }
   ],
   "source": [
    "# F1-score\n",
    "print(f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  177   606  1170    52     3]\n",
      " [  168  1779  7030   250    28]\n",
      " [   50   595 54967   856    80]\n",
      " [    7   131  7347  3194   319]\n",
      " [    0    29  1506  1616   640]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The misclassifications show where the problem lies. The class imbalance is causing the classifier to make more 'neutral' sentiment predictions. Even for more extreme values, the classifications error towards neutral state.\n",
    "\n",
    "The weighted F1 Score gives a good overall measure to directly evaluate the models, the other is log loss as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8609283543125711\n"
     ]
    }
   ],
   "source": [
    "# Log loss per sample\n",
    "print(log_loss(y_true, y_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing imbalance with oversampling\n",
    "\n",
    "To reduce misclassification due to imbalance, we try oversampling the minority classes. Undersampling would not be a good choice as the vocabulary matrix is sparse and this will result in model classifying most of the 1-grams not seen as neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Rebalance: Counter({2: 56548, 3: 56548, 1: 56548, 4: 56548, 0: 56548})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_os, y_os = ros.fit_resample(X_data_test_vec, y_data_test.astype(int))\n",
    "\n",
    "print('After Rebalance: {0}'.format(Counter(y_os)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "nb_rebal_model = MultinomialNB()\n",
    "nb_rebal_model.fit(X_os, y_os)\n",
    "\n",
    "# Score model after rebalance\n",
    "y_pred = nb_rebal_model.predict(X_data_test_vec)\n",
    "y_true = y_data_test.astype(int)\n",
    "y_probs = nb_rebal_model.predict_proba(X_data_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on full test data (NB):     0.338172\n"
     ]
    }
   ],
   "source": [
    "#print(\"Accuracy on training data: {:2f}\".format(training_accuracy))\n",
    "print(\"Accuracy on full test data (NB):     {:2f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling does not improve the accuracy of the classification at all. We will use the original unbalanced sample as the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root Level Evaluation Metrics\n",
    "\n",
    "Next we examine how the root level accuracy and classification metrics are, which gives us the overall prediction for a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build root train data set\n",
    "x_root_train = [tree.text() for tree in x_train]\n",
    "y_root_train = [tree.root.label for tree in x_train]\n",
    "x_root_dev = [tree.text() for tree in x_dev]\n",
    "y_root_dev = [tree.root.label for tree in x_dev]\n",
    "x_root_all = x_root_train + x_root_dev\n",
    "y_root_all = y_root_train + y_root_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize x\n",
    "x_root_train_vec = vectorizer.transform(x_root_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model for root nodes\n",
    "nb_root = MultinomialNB()\n",
    "nb_root.fit(x_root_train_vec, y_root_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build root test data set\n",
    "x_root_test = [tree.text() for tree in x_test]\n",
    "y_root_test = [tree.root.label for tree in x_test]\n",
    "x_root_test_vec = vectorizer.transform(x_root_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on root test data (NB):     0.319457\n"
     ]
    }
   ],
   "source": [
    "# Score model\n",
    "# Print the accuracy on the test dataset\n",
    "y_pred = nb_model.predict(x_root_test_vec)\n",
    "y_true = y_root_test\n",
    "y_probs = nb_model.predict_proba(x_root_test_vec)\n",
    "\n",
    "print(\"Accuracy on root test data (NB):     {:2f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.11      0.18       279\n",
      "           1       0.51      0.23      0.32       633\n",
      "           2       0.21      0.75      0.33       389\n",
      "           3       0.40      0.32      0.36       510\n",
      "           4       0.66      0.19      0.30       399\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      2210\n",
      "   macro avg       0.49      0.32      0.30      2210\n",
      "weighted avg       0.48      0.32      0.31      2210\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3088168253391177\n"
     ]
    }
   ],
   "source": [
    "# F1-score\n",
    "print(f1_score(y_true, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 30  78 165   5   1]\n",
      " [ 13 147 435  33   5]\n",
      " [  3  37 290  54   5]\n",
      " [  0  19 300 163  28]\n",
      " [  0   5 165 153  76]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4381252006114305\n"
     ]
    }
   ],
   "source": [
    "# Log loss per sample\n",
    "print(log_loss(y_true, y_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root level accuracy is much lower as expected. This also aligns with a max accuracy of about 45% for the best model in the paper.\n",
    "\n",
    "Here, too we see distinct effect of too many neutral words on the overall accuracy of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
