{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the data exploration phase, we observed that longer n-gram length is associated with a presence of a sentiment. However, bag of words kind of models fail to capture compositional effects associated with sentence structure such as sentence negation. The Recursive Neural Tensor Network Model (RNTN) is a recursive neural network model that captures these compositional effects by relying on constituency parsed representation of the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model computes word vector representations for each word in the vocabulary and generates similar word representations for intermediate nodes that are recursively used for predictions for the root node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following trigram example shows how the prediction occurs at each phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](TrigramExample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word vector is represented as a $d$ dimensional word vector. All the word vectors are stacked in the word embedding matrix $L$ of dimensions \\[d, V\\], where V is the size of the vocabulary. The word vectors are initialized from a random uniform distribution in interval \\[-0.0001, 0.0001\\], and the L matrix is seen as a parameter that is trained jointly with\n",
    "the compositionality models.\n",
    "\n",
    "A word vector for every intermediate node in the tree, that is not a leaf, is a bi-gram and is computed recursively from its children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Composition step can be represented by the following equations:\n",
    "\n",
    "> $zs = W*X + b$ (Standard term)\n",
    "\n",
    "> $zt = X^{T} * T * X$ (Neural Tensor term)\n",
    "\n",
    "> $a = f(zs + zt)$ (Composition function)\n",
    "\n",
    "where:\n",
    "* W: Weights to be computed by the model of shape \\[$d$, 2*$d$\\] for Composition step.\n",
    "* X: Word embeddings for input words stacked together.  X is a column vector of shape \\[2*$d$, 1\\]\n",
    "* b: bias term for the node of shape \\[$d$, 1\\]\n",
    "* T: Tensor of dimension \\[2*$d$, 2*$d$, $d$\\]. Each T\\[:,:,$i$\\] slice generates a scalar, which is one component of the final word vector of dimension d.\n",
    "* f: Non-linear function specifying the compositionality of the classifier. *tanh* in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main benefit of this model is due to the tensor layer. This layer generates internal representations for the most common tree structures, and removes the need to maintain contextual representations for intermediate nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated word vectors are used as parameters to optimize and as feature inputs to a softmax classifier to project weights to sentiment probabilities. For classification into five classes, we compute the posterior probability over labels given the word vector via:\n",
    "\n",
    "> y = $U^{T} * a + bs$\n",
    "\n",
    "where,\n",
    "* U: Sentiment Classification Matrix Weights to be computed by model for Projection Step of shape \\[d, label_size\\] where label_size is the number of classes.\n",
    "* bs: Bias term for Projection Step of shape \\[label_size, 1\\]\n",
    "* a: Output of the composition layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above example, let\n",
    "\n",
    "> $b$ = Word vector embedding of 'very'\n",
    "\n",
    "> $c$ = Word vector embedding of 'good'\n",
    "\n",
    "> $p1$ = Composed Word vector for phrase 'very good'\n",
    "\n",
    "The vector is composed as:\n",
    "\n",
    "> $p1 = f\\left( \\left[ \\begin{matrix} b \\\\ c \\end{matrix} \\right]^{T} T^{[1:d]} \\left[ \\begin{matrix} b \\\\ c \\end{matrix} \\right] + W \\left[ \\begin{matrix} b \\\\ c \\end{matrix} \\right] \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the final sentiment of the phrase 'not very good' is computed recursively using word embedding for 'not' and $p1$. Let,\n",
    "\n",
    "> $a$ = Word vector embedding of 'not'\n",
    "\n",
    "> $p2$ = Composed Word vector for phrase 'not very good'\n",
    "\n",
    "> $p2 = f\\left( \\left[ \\begin{matrix} a \\\\ p1 \\end{matrix} \\right]^{T} T^{[1:d]} \\left[ \\begin{matrix} a \\\\ p1 \\end{matrix} \\right] + W \\left[ \\begin{matrix} a \\\\ p1 \\end{matrix} \\right] \\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the optimizer is to maximize the prediction or minimize the cross-entropy error between the predicted distribution $y_{i}$ and the target distribution $t_{i}$. This is equivalent (up to a constant) to minimizing the KL-divergence between the two distributions. The error ($E$) as a function of the RNTN parameters,\n",
    "\n",
    "> $\\theta = (L, W, U, T)$\n",
    "\n",
    "> $E(\\theta) = \\sum_{i} \\sum_{j} t_{j}^{i} \\log y_{j}^{i} + \\lambda||\\theta||^{2}$\n",
    "\n",
    "where\n",
    "\n",
    "> $i$ = index of every node in the tree.\n",
    "\n",
    "> $j$ = index of every class in the label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use L2 regularizer here as it is more computationally efficient and we do not need feature selection.\n",
    "\n",
    "The optimization is done using AdaGrad optimizer as it adapts the learning rate to the parameters, performing smaller updates\n",
    "(i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back-progagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For understanding back-propagation, we start by looking at the derivative of the loss function at Projection and Composition steps for all nodes with respect to parameters $(U, W, T)$.\n",
    "\n",
    "The derivative of the loss function at with respect to $U$ is the softmax cross-entropy error, which is simple the sum of each node error, that is,\n",
    "\n",
    "> $\\delta_{i, s} = U^{T}(y_i - t_i) \\otimes f'(x_i) $ \n",
    "\n",
    "where,\n",
    "\n",
    "$\\delta_{i, s}$ is the softmax error at Projection Layer.\n",
    "\n",
    "$y_i$ is the ground truth label.\n",
    "\n",
    "$t_i$ is the predicted softmax probability.\n",
    "\n",
    "$x_i$ is the vector from the Composition layer.\n",
    "\n",
    "$f'$ is the derivative of tanh and is given by $f'(x) = 1 - f(x)^2$.\n",
    "\n",
    "$\\otimes$ indicates a *Hadamard* product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at how error changes with respect to Composition Layer weights $W$ and $T$.\n",
    "\n",
    "The error due to Composition Layer changes depending on which node we are looking at. For the root node, this value is the softmax error from the Projection Layer. For other nodes, this error can only be computed in a top-down fashion from root node to the leaves.\n",
    "\n",
    "Let $\\delta_{i, com}$ be the incoming error vector at node $i$. For the root node $\\delta_{p2, com} = \\delta_{p2, s}$. This can be used to compute the standard derivative with respect to $W$ as $W^{T}\\delta_{p2, com}$.\n",
    "\n",
    "Similarly, the derivative with respect to T can be obtained by looking at each tensor slice for $k=1,...,d$ as,\n",
    "> $\\frac{\\partial E^{p2}}{\\partial V^{[k]}} = \\delta_{p2, com} \\left[ \\begin{matrix} a \\\\ p1 \\end{matrix} \\right] \\left[ \\begin{matrix} a \\\\ p1 \\end{matrix} \\right]^{T}$ \n",
    "\n",
    "The total derivative for the error with respect to $W$ and $T$ at node $p2$ becomes,\n",
    "> $\\delta_{p2, out} = \\left( W^{T} \\delta_{p2, com} + S \\right) \\otimes f' \\left( \\left[ \\begin{matrix} a \\\\ p1 \\end{matrix} \\right] \\right)$\n",
    "\n",
    "where,\n",
    "\n",
    "> $S = \\sum_{k=1}^{d} \\delta_{p2, com}^k \\big( V^{[k]} + (V^{[k]})^T \\big) \\left[ \\begin{matrix} a \\\\ p1 \\end{matrix} \\right]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The children of p2, will then each take half of this vector and add their own softmax error message for the complete $\\delta$. In particular, we have for $p1$,\n",
    "\n",
    "> $\\delta_{p1, com} = \\delta(p1, s) + \\delta_{p2, out}\\big[ d+1:2d \\big]$,\n",
    "\n",
    "where,\n",
    "\n",
    "> $[d+1:2d]$ represents the vector corresponding to the right child.\n",
    "\n",
    "The full derivative is the sum of derivatives at all nodes, or\n",
    "\n",
    "> $\\frac{\\partial E}{\\partial V^[k]} = \\sum_{i} \\sum_{k=1}{d} \\delta_{i, com}^k$.\n",
    "\n",
    "The derivative of W can be computed in exactly similar way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
